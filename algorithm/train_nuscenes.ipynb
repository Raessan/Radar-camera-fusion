{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_nuscenes import DepthDatasetNuscenes\n",
    "from model import RadarCamModel\n",
    "from loss import custom_loss, depth_loss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from plotting import draw_result, draw_error_map\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DATASET PARAMETERS\n",
    "DIR_DATA = '/ssd/Datasets_and_code/nuscenes_depth_estimation/dataset/dataset_radar_cam' # Root to the folder with the prepared data\n",
    "W_RESIZED = 800 #800 # If 0, the image is not resized in width\n",
    "H_RESIZED = 450 #450 # If 0, the image is not resized in height\n",
    "MIN_DIST = 0.0 # Threshold minimum depth to discard points\n",
    "MAX_DIST = 50.0 # Threshold maximum depth to discard points\n",
    "NUMBER_RADAR_POINTS = 100 # Number of radar points to use. If the number of points in a sample is less than this, they are \n",
    "                          # resampled randomly to create an array of fixed number of points. This shouldn't hurt the performance\n",
    "AUGMENT_DIST = True\n",
    "AUGMENT_DIST_MAX_SUBTRACT = 5.0\n",
    "AUGMENT_DIST_MIN_SCALE = 0.5\n",
    "    \n",
    "# MODEL PARAMETERS\n",
    "BATCH_SIZE = 32\n",
    "RADAR_CHANNELS_ENCODER = [64, 128, 256, 432] \n",
    "UNET_CHANNELS_IMG = [2, 4, 8, 12, 16, 24, 32, 48] \n",
    "UNET_CHANNELS_RADAR = 24\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "L2_LIDAR = False\n",
    "ALPHA_LOSS = 1.0\n",
    "BETA_LOSS = 5.0\n",
    "GAMMA_LOSS = 0.0\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 2\n",
    "NUMBER_SAMPLES_PLOT = 3\n",
    "LOAD_MODEL = False\n",
    "MODEL_PATH = 'results/2024-05-22_22-49-26/model_14.pth'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "train_set = DepthDatasetNuscenes(\"train\", DIR_DATA, W_RESIZED, H_RESIZED, MIN_DIST, MAX_DIST, NUMBER_RADAR_POINTS, \n",
    "                                 AUGMENT_DIST, AUGMENT_DIST_MAX_SUBTRACT, AUGMENT_DIST_MIN_SCALE)\n",
    "train_dataloader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_set = DepthDatasetNuscenes(\"val\", DIR_DATA, W_RESIZED, H_RESIZED, MIN_DIST, MAX_DIST, NUMBER_RADAR_POINTS)\n",
    "val_dataloader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = RadarCamModel(RADAR_CHANNELS_ENCODER, UNET_CHANNELS_IMG, UNET_CHANNELS_RADAR).to(device)\n",
    "model.required_radar_size(torch.from_numpy(np.expand_dims(train_set.__getitem__(0)[1], axis=0)).to(device, dtype=torch.float))\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "n_params_radar = sum([p.numel() for p in model.radar_encoder.parameters()])\n",
    "n_params_unet = sum([p.numel() for p in model.unet.parameters()])\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    print(\"Model successfully loaded!\")\n",
    "\n",
    "print(\"Total number of parameters: \", n_params)\n",
    "print(\"UNet number of parameters: \", n_params_unet)\n",
    "print(\"Radar number of parameters: \", n_params_radar)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Define variables to track the best validation loss and corresponding model parameters\n",
    "val_losses = []\n",
    "model_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##################### TRAIN #######################\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (sample_token, rel_depth, radar, lidar) in enumerate(tqdm(train_dataloader)):\n",
    "        rel_depth, radar, lidar = rel_depth.to(device, dtype=torch.float), radar.to(device, dtype=torch.float), lidar.to(device, dtype=torch.float)\n",
    "           \n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(rel_depth, radar)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = custom_loss(output, lidar, rel_depth, l2_lidar=L2_LIDAR, alpha=ALPHA_LOSS, beta=BETA_LOSS, gamma=GAMMA_LOSS)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss[0].backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss[0].item()\n",
    "\n",
    "        if (batch_idx % 50 == 0):\n",
    "            print(f\"Loss lidar: {loss[1]}, loss smoothness: {loss[2]}, loss consistency: {loss[3]}\")\n",
    "            print(f\"Epoch {epoch+1}, batch {batch_idx}, Loss: {train_loss/(batch_idx+1)}\")\n",
    "\n",
    "    train_loss /= float(batch_idx+1)\n",
    "     \n",
    "        \n",
    "    ##################### VALIDATION #######################\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loss_lidar = 0.0\n",
    "    val_loss_smoothness = 0.0\n",
    "    val_loss_consistency = 0.0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sample_token, rel_depth, radar, lidar) in enumerate(tqdm(val_dataloader)):\n",
    "            rel_depth, radar, lidar = rel_depth.to(device, dtype=torch.float), radar.to(device, dtype=torch.float), lidar.to(device, dtype=torch.float)\n",
    "\n",
    "            output = model(rel_depth, radar)\n",
    "            \n",
    "            loss = custom_loss(output, lidar, rel_depth, l2_lidar=L2_LIDAR, alpha=ALPHA_LOSS, beta=BETA_LOSS, gamma=GAMMA_LOSS)\n",
    "            val_loss += loss[0].item()\n",
    "            val_loss_lidar += loss[1].item()\n",
    "            val_loss_smoothness += loss[2].item()\n",
    "            val_loss_consistency += loss[3].item()\n",
    "            \n",
    "            if (batch_idx==0):\n",
    "                rel_depth_sample = rel_depth.cpu()[:NUMBER_SAMPLES_PLOT,:, :, :]\n",
    "                lidar_sample = lidar.cpu()[:NUMBER_SAMPLES_PLOT,:, :, :]\n",
    "                output_sample = output.cpu()[:NUMBER_SAMPLES_PLOT,:, :, :]\n",
    "                draw_result(rel_depth_sample, output_sample, lidar_sample, MIN_DIST, MAX_DIST)\n",
    "\n",
    "        val_loss /= float(batch_idx+1)\n",
    "        val_loss_lidar /= float(batch_idx+1)\n",
    "        val_loss_smoothness /= float(batch_idx+1)\n",
    "        val_loss_consistency /= float(batch_idx+1)\n",
    "    \n",
    "    # Print average loss at the end of each epoch\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss}, val loss NN total: {val_loss}, val loss NN lidar: {val_loss_lidar},  val loss NN smoothness: {val_loss_smoothness}, val loss NN consistency: {val_loss_consistency}\")\n",
    "    \n",
    "    val_losses.append(val_loss)\n",
    "    model_params.append(copy.deepcopy(model.state_dict()))\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the model\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "folder_path = f\"results/{current_date}\"\n",
    "os.mkdir(folder_path) \n",
    "json_path = os.path.join(folder_path,\"params.json\")\n",
    "\n",
    "json_params = { \n",
    "    \"W_RESIZED\" : W_RESIZED, \n",
    "    \"H_RESIZED\" : H_RESIZED, \n",
    "    \"MIN_DIST\" : MIN_DIST, \n",
    "    \"MAX_DIST\" : MAX_DIST,\n",
    "    \"NUMBER_RADAR_POINTS\" : NUMBER_RADAR_POINTS,\n",
    "    \"RADAR_CHANNELS_ENCODER\" : RADAR_CHANNELS_ENCODER,\n",
    "    \"UNET_CHANNELS_IMG\" : UNET_CHANNELS_IMG,\n",
    "    \"UNET_CHANNELS_RADAR\" : UNET_CHANNELS_RADAR,\n",
    "    \"L2_LIDAR\" : L2_LIDAR,\n",
    "    \"ALPHA_LOSS\" : ALPHA_LOSS,\n",
    "    \"BETA_LOSS\" : BETA_LOSS,\n",
    "    \"GAMMA_LOSS\" : GAMMA_LOSS,\n",
    "    \"LEARNING_RATE\" : LEARNING_RATE,\n",
    "    \"LOAD_MODEL\" : LOAD_MODEL,\n",
    "    \"MODEL_PATH\" : MODEL_PATH,\n",
    "    \"val_losses\" : val_losses\n",
    "} \n",
    "\n",
    "# Save the model state dictionary\n",
    "# Finding inimum\n",
    "min_error = min(val_losses)\n",
    " \n",
    "# iterating over the list like index,\n",
    "# item pair\n",
    "print(val_losses)\n",
    "for i in range(len(model_params)):\n",
    "    model_path = os.path.join(folder_path,f\"model_{i}.pth\")\n",
    "    torch.save(model_params[i], model_path)\n",
    "        \n",
    "with open(json_path, \"w\") as outfile: \n",
    "    json.dump(json_params, outfile)\n",
    "\n",
    "print(f\"Model saved to {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1cf23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
